{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c613402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "import os\n",
    "import shutil\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "current_file_dir = os.getcwd()\n",
    "\n",
    "root_dir = os.path.dirname(current_file_dir)\n",
    "\n",
    "sys.path.append(root_dir)\n",
    "dataset_dir = os.path.join(root_dir, 'data')\n",
    "\n",
    "\n",
    "processed_dir = os.path.join(dataset_dir, 'processed')\n",
    "if not os.path.exists(processed_dir):\n",
    "    os.makedirs(processed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf584f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def generate_occluded_dataset_dir(org_dataset_dir, occluded_dataset_name='occluded', occulude_ratio=0.6):\n",
    "    mask_dir = os.path.join(org_dataset_dir, 'masked')\n",
    "    cam_dir = os.path.join(org_dataset_dir, 'cam')\n",
    "    pc_dir = os.path.join(org_dataset_dir, 'pc')\n",
    "\n",
    "    csv_path = os.path.join(org_dataset_dir, 'trainval.csv')\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"CSV file not found at {csv_path}\")\n",
    "\n",
    "    train_validate_df = pd.read_csv(csv_path)\n",
    "    trainable_files = train_validate_df[train_validate_df['trainable'] == 1]['filename'].tolist()\n",
    "\n",
    "    if not trainable_files:\n",
    "        raise ValueError(\"No trainable files found in CSV.\")\n",
    "\n",
    "    total_samples = len(trainable_files)\n",
    "    num_occluded = int(total_samples * occulude_ratio)\n",
    "    occlude_indices = random.sample(range(total_samples), num_occluded)\n",
    "\n",
    "    # Create occluded dataset directory (one level up from original)\n",
    "    occluded_dataset_dir = os.path.join(os.path.dirname(org_dataset_dir), occluded_dataset_name)\n",
    "    occluded_cam_dir = os.path.join(occluded_dataset_dir, 'cam')\n",
    "    occluded_pc_dir = os.path.join(occluded_dataset_dir, 'pc')\n",
    "    occluded_masked_dir = os.path.join(occluded_dataset_dir, 'masked')\n",
    "\n",
    "    os.makedirs(occluded_cam_dir, exist_ok=True)\n",
    "    os.makedirs(occluded_pc_dir, exist_ok=True)\n",
    "    os.makedirs(occluded_masked_dir, exist_ok=True)\n",
    "\n",
    "    # Sample image to determine size\n",
    "    sample_image_path = os.path.join(cam_dir, trainable_files[0])\n",
    "    if not os.path.exists(sample_image_path):\n",
    "        raise FileNotFoundError(f\"Sample image not found at {sample_image_path}\")\n",
    "    try:\n",
    "        image = Image.open(sample_image_path).convert('RGB')\n",
    "    except Exception as e:\n",
    "        shutil.rmtree(occluded_dataset_dir)\n",
    "        raise RuntimeError(f\"Error reading sample image {sample_image_path}\") from e\n",
    "\n",
    "    empty_image = Image.new('RGB', image.size, (0, 0, 0))\n",
    "    processed_files = []\n",
    "\n",
    "    try:\n",
    "        for idx in occlude_indices:\n",
    "            original_name = trainable_files[idx]\n",
    "            base_name, ext = os.path.splitext(original_name)\n",
    "            occluded_name = f\"{base_name}_occluded{ext}\"\n",
    "\n",
    "            # Paths to original files\n",
    "            cam_path = os.path.join(cam_dir, original_name)\n",
    "            pc_path = os.path.join(pc_dir, original_name)\n",
    "            mask_path = os.path.join(mask_dir, original_name)\n",
    "            for path in [cam_path, pc_path, mask_path]:\n",
    "                if not os.path.exists(path):\n",
    "                    raise FileNotFoundError(f\"Required file not found: {path}\")\n",
    "\n",
    "            # Random occlusion target for each file\n",
    "            target_to_occlude = random.choice(['cam', 'point_cloud'])\n",
    "\n",
    "            if target_to_occlude == 'cam':\n",
    "                empty_image.save(os.path.join(occluded_cam_dir, occluded_name))\n",
    "                shutil.copy(pc_path, os.path.join(occluded_pc_dir, occluded_name))\n",
    "            elif target_to_occlude == 'point_cloud':\n",
    "                shutil.copy(cam_path, os.path.join(occluded_cam_dir, occluded_name))\n",
    "                empty_image.save(os.path.join(occluded_pc_dir, occluded_name))\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid occlusion target: {target_to_occlude}\")\n",
    "\n",
    "            shutil.copy(mask_path, os.path.join(occluded_masked_dir, occluded_name))\n",
    "            processed_files.append(occluded_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        shutil.rmtree(occluded_dataset_dir, ignore_errors=True)\n",
    "        raise RuntimeError(f\"Failed while processing occluded dataset: {str(e)}\") from e\n",
    "\n",
    "    return occluded_dataset_dir, sorted(processed_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050a2205",
   "metadata": {},
   "outputs": [],
   "source": [
    "occluded_dataset_dir, processed_files = generate_occluded_dataset_dir(\n",
    "    org_dataset_dir=os.path.join(processed_dir, 'lidarseg_images', 'trainval'),\n",
    "    occluded_dataset_name='occluded',\n",
    "    occulude_ratio=0.6\n",
    ")\n",
    "print(f\"Occluded dataset created at: {occluded_dataset_dir}\")\n",
    "print(f\"Processed files: {processed_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a560ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processed_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99594d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## random split train and val\n",
    "train_files = random.sample(processed_files, int(len(processed_files) * 0.8))\n",
    "val_files = list(set(processed_files) - set(train_files))\n",
    "train_df = pd.DataFrame({'filename': train_files, 'trainable': 1})\n",
    "val_df = pd.DataFrame({'filename': val_files, 'trainable': 0})\n",
    "\n",
    "#merge train and val df\n",
    "trainval_df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "trainval_df.to_csv(os.path.join(occluded_dataset_dir, 'trainval.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d02139af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 34149\n",
      "Trainable samples: 27319\n",
      "Non-trainable samples: 6830\n"
     ]
    }
   ],
   "source": [
    "base_dataset_dir = os.path.join(processed_dir, 'lidarseg_images', 'trainval')\n",
    "\n",
    "df = pd.read_csv(os.path.join(base_dataset_dir, 'trainval.csv'))\n",
    "\n",
    "## report dataset statistics\n",
    "def report_dataset_statistics(df):\n",
    "    trainable_count = df[df['trainable'] == 1].shape[0]\n",
    "    non_trainable_count = df[df['trainable'] == 0].shape[0]\n",
    "    print(f\"Total samples: {df.shape[0]}\")\n",
    "    print(f\"Trainable samples: {trainable_count}\")\n",
    "    print(f\"Non-trainable samples: {non_trainable_count}\")\n",
    "\n",
    "\n",
    "\n",
    "report_dataset_statistics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f92414a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 34149\n",
      "Trainable samples: 27319\n",
      "Non-trainable samples: 6830\n"
     ]
    }
   ],
   "source": [
    "occluded_dataset_dir = os.path.join(processed_dir, 'lidarseg_images', 'occcluded')\n",
    "\n",
    "df = pd.read_csv(os.path.join(base_dataset_dir, 'trainval.csv'))\n",
    "\n",
    "## report dataset statistics\n",
    "def report_dataset_statistics(df):\n",
    "    trainable_count = df[df['trainable'] == 1].shape[0]\n",
    "    non_trainable_count = df[df['trainable'] == 0].shape[0]\n",
    "    print(f\"Total samples: {df.shape[0]}\")\n",
    "    print(f\"Trainable samples: {trainable_count}\")\n",
    "    print(f\"Non-trainable samples: {non_trainable_count}\")\n",
    "\n",
    "\n",
    "\n",
    "report_dataset_statistics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0c4ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
