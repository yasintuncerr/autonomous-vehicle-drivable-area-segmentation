{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79f002b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasin/Lfstorage/anaconda3/envs/pytorch/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/yasin/Lfstorage/anaconda3/envs/pytorch/lib/python3.13/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "\n",
    "current_file_dir = os.getcwd()\n",
    "\n",
    "root_dir = os.path.dirname(current_file_dir)\n",
    "\n",
    "sys.path.append(root_dir)\n",
    "dataset_dir = os.path.join(root_dir, 'data')\n",
    "\n",
    "\n",
    "processed_dir = os.path.join(dataset_dir, 'processed')\n",
    "if not os.path.exists(processed_dir):\n",
    "    os.makedirs(processed_dir)\n",
    "\n",
    "\n",
    "from src.utils.preprocess import create_pointcloud_image, create_morphological_polygon\n",
    "from src.model import UNetModel, UNetConfig, create_unet_model\n",
    "from src.utils.dataset import MultiViewImageDataset, JustCAM\n",
    "from src.utils.loss import DiceLoss, FocalLoss, CombinedLoss, IoULoss\n",
    "from src.utils.metrics import SegmentationMetrics\n",
    "from torchvision import transforms\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20d1261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c44a5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = os.path.join(dataset_dir, 'processed', 'lidarseg_images')\n",
    "input_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    #convert RGB to grayscale\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "batch_size = 24\n",
    "num_workers = 16\n",
    "image_size = (398,224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f0ad63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal dataset size: 27319\n",
      "Validation dataset size: 6830\n",
      "Occluded dataset size: 13112\n",
      "Validation dataset size: 3279\n",
      "Train dataset size: 67750\n",
      "Validation dataset size: 16939\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "normal_dataset = MultiViewImageDataset(\n",
    "    root_dir=os.path.join(dataset_dir, 'trainval'),\n",
    "    input_transform=input_transform,\n",
    "    mask_transform=mask_transform,\n",
    "    image_size=image_size)\n",
    "\n",
    "train_valid_df = pd.read_csv(os.path.join(dataset_dir,'trainval', 'trainval.csv'))\n",
    "trainables = train_valid_df[train_valid_df['trainable'] == 1]['filename'].tolist()\n",
    "validables = train_valid_df[train_valid_df['trainable'] == 0]['filename'].tolist()\n",
    "\n",
    "train_normal_dataset = copy.deepcopy(normal_dataset)\n",
    "valid_normal_dataset = copy.deepcopy(normal_dataset)\n",
    "\n",
    "train_normal_dataset.update_image_names(trainables)\n",
    "valid_normal_dataset.update_image_names(validables)\n",
    "\n",
    "print(f\"Normal dataset size: {len(train_normal_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(valid_normal_dataset)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "just_cam_dataset = JustCAM(\n",
    "    root_dir=os.path.join(dataset_dir, 'trainval'),\n",
    "    input_transform=input_transform,\n",
    "    mask_transform=mask_transform,\n",
    "    image_size=image_size)\n",
    "\n",
    "\n",
    "valid_just_cam_dataset = copy.deepcopy(just_cam_dataset)\n",
    "train_just_cam_dataset = copy.deepcopy(just_cam_dataset)\n",
    "train_just_cam_dataset.update_image_names(trainables)\n",
    "valid_just_cam_dataset.update_image_names(validables)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "occluded_dataset = MultiViewImageDataset(\n",
    "    root_dir=os.path.join(dataset_dir, 'occluded'),\n",
    "    input_transform=input_transform,\n",
    "    mask_transform=mask_transform,\n",
    "    image_size=image_size)\n",
    "train_valid_occluded_df = pd.read_csv(os.path.join(dataset_dir,'occluded', 'trainval.csv'))\n",
    "trainables_occluded = train_valid_occluded_df[train_valid_occluded_df['trainable'] == 1]['filename'].tolist()\n",
    "validables_occluded = train_valid_occluded_df[train_valid_occluded_df['trainable'] == 0]['filename'].tolist()\n",
    "train_occluded_dataset = copy.deepcopy(occluded_dataset)\n",
    "valid_occluded_dataset = copy.deepcopy(occluded_dataset)\n",
    "train_occluded_dataset.update_image_names(trainables_occluded)\n",
    "valid_occluded_dataset.update_image_names(validables_occluded)\n",
    "print(f\"Occluded dataset size: {len(train_occluded_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(valid_occluded_dataset)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## merge datasets\n",
    "train_dataset = train_normal_dataset + train_occluded_dataset + train_just_cam_dataset\n",
    "valid_dataset = valid_normal_dataset + valid_occluded_dataset + valid_just_cam_dataset\n",
    "\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(valid_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9801e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ee3faa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_unet_model(\n",
    "    in_channels=6,\n",
    "    out_channels=1,\n",
    "    input_height=224,\n",
    "    input_width=398\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ba8dbdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNetModel(\n",
       "  (ups): ModuleList(\n",
       "    (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (3): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (4): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (5): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (7): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (downs): ModuleList(\n",
       "    (0): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(6, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bottleneck): DoubleConv(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (final_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#device \n",
    "device = \"cuda\"\n",
    "\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dbf8b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "criterion = CombinedLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17da6ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 -> Train Loss: 0.1394, Val Loss: 0.0817\n",
      "Val Metrics -> IoU: 0.7678, Dice: 0.8645, Precision: 0.8526, Recall: 0.8799, F1: 0.8645, PixelAcc: 0.8486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 -> Train Loss: 0.0772, Val Loss: 0.0776\n",
      "Val Metrics -> IoU: 0.7681, Dice: 0.8642, Precision: 0.8661, Recall: 0.8653, F1: 0.8642, PixelAcc: 0.8494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 -> Train Loss: 0.0702, Val Loss: 0.0696\n",
      "Val Metrics -> IoU: 0.7830, Dice: 0.8756, Precision: 0.8714, Recall: 0.8819, F1: 0.8756, PixelAcc: 0.8509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 -> Train Loss: 0.0669, Val Loss: 0.0686\n",
      "Val Metrics -> IoU: 0.7865, Dice: 0.8779, Precision: 0.8684, Recall: 0.8892, F1: 0.8779, PixelAcc: 0.8509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 -> Train Loss: 0.0647, Val Loss: 0.0657\n",
      "Val Metrics -> IoU: 0.7922, Dice: 0.8821, Precision: 0.8740, Recall: 0.8916, F1: 0.8821, PixelAcc: 0.8515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 -> Train Loss: 0.0628, Val Loss: 0.0631\n",
      "Val Metrics -> IoU: 0.7960, Dice: 0.8846, Precision: 0.8839, Recall: 0.8868, F1: 0.8846, PixelAcc: 0.8520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 -> Train Loss: 0.0613, Val Loss: 0.0617\n",
      "Val Metrics -> IoU: 0.8009, Dice: 0.8880, Precision: 0.8810, Recall: 0.8960, F1: 0.8880, PixelAcc: 0.8521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 -> Train Loss: 0.0602, Val Loss: 0.0610\n",
      "Val Metrics -> IoU: 0.8022, Dice: 0.8889, Precision: 0.8819, Recall: 0.8968, F1: 0.8889, PixelAcc: 0.8523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 -> Train Loss: 0.0589, Val Loss: 0.0618\n",
      "Val Metrics -> IoU: 0.8005, Dice: 0.8878, Precision: 0.8786, Recall: 0.8980, F1: 0.8878, PixelAcc: 0.8523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 -> Train Loss: 0.0583, Val Loss: 0.0596\n",
      "Val Metrics -> IoU: 0.8048, Dice: 0.8906, Precision: 0.8865, Recall: 0.8955, F1: 0.8906, PixelAcc: 0.8525\n"
     ]
    }
   ],
   "source": [
    "# --- Eğitim Döngüsü ---\n",
    "EPOCH = 10\n",
    "# SegmentationMetrics sınıfından bir nesne oluştur\n",
    "metrics_calculator = SegmentationMetrics(threshold=0.5) # Eşik değerini ayarlayabilirsiniz\n",
    "\n",
    "best_val_metric = 0.0 # Veya en düşük kayıp için float('inf')\n",
    "best_model_dir = \"output\"\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    # --- Eğitim Aşaması ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCH} - Training\", leave=False)\n",
    "    for batch in train_progress_bar:\n",
    "        cam = batch[\"cam\"].to(device)\n",
    "        pointcloud = batch[\"point_cloud\"].to(device)\n",
    "\n",
    "        ## concatenate cam and pointcloud should be 6 channels\n",
    "        inputs = torch.cat((cam, pointcloud), dim=1) \n",
    "        masks = batch[\"masked\"].to(device)\n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # --- Doğrulama (Validation) Aşaması ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    metrics_calculator.reset() # Her epoch başında metrikleri sıfırla\n",
    "\n",
    "    val_progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCH} - Validation\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in val_progress_bar:\n",
    "            \n",
    "            cam = batch[\"cam\"].to(device)\n",
    "            pointcloud = batch[\"point_cloud\"].to(device)\n",
    "            ## concatenate cam and pointcloud should be 6 channels\n",
    "            inputs = torch.cat((cam, pointcloud), dim=1)\n",
    "            masks = batch[\"masked\"].to(device)\n",
    "\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, masks) # Validation loss'u hesapla\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Metrikleri güncelle\n",
    "            metrics_calculator.update(outputs, masks) # Model çıktıları (logitler) ve hedefler\n",
    "            val_progress_bar.set_postfix(loss=loss.item())\n",
    "        \n",
    "\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    epoch_metrics = metrics_calculator.compute() # Epoch için ortalama metrikleri al\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCH} -> Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Val Metrics -> IoU: {epoch_metrics['IoU']:.4f}, Dice: {epoch_metrics['Dice']:.4f}, \"\n",
    "          f\"Precision: {epoch_metrics['Precision']:.4f}, Recall: {epoch_metrics['Recall']:.4f}, \"\n",
    "          f\"F1: {epoch_metrics['F1']:.4f}, PixelAcc: {epoch_metrics['PixelAcc']:.4f}\")\n",
    "\n",
    "    # Scheduler'ı güncelle (eğer validation loss'a bağlıysa)\n",
    "    # Eğer ReduceLROnPlateau gibi bir scheduler kullanıyorsanız:\n",
    "    scheduler.step(val_loss)\n",
    "    # Eğer CosineAnnealingLR gibi epoch bazlı bir scheduler kullanıyorsanız:\n",
    "    # scheduler.step()\n",
    "\n",
    "    # En iyi modeli kaydet\n",
    "    if epoch_metrics['IoU'] > best_val_metric:\n",
    "        best_val_metric = epoch_metrics['IoU']\n",
    "        model.save_pretrained(os.path.join(best_model_dir, f\"lidarseg_unet_epoch_{epoch+1}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d29a89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = os.path.join(root_dir, 'models')\n",
    "if not os.path.exists(model_save_dir):\n",
    "    os.makedirs(model_save_dir)\n",
    "\n",
    "model_save_path = os.path.join(model_save_dir, 'lidarseg_unet-aug')\n",
    "\n",
    "model.save_pretrained(model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
